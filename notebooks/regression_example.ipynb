{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import spectralgp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the GPyTorch Model with Spectral GP kernel\n",
    "Using the same framework as standard GPyTorch models, we merely drop-in the spectral GP kernel as the covar module\n",
    "\n",
    "The `initialize_from_data` method does some pre-training on the latent model using the log-periodogram of data as training targets.\n",
    "\n",
    "For specifics on the components of GPyTorch models we refer to the [GPyTorch Documentation](https://gpytorch.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, **kwargs):\n",
    "        super(SpectralModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = spectralgp.kernels.SpectralGPKernel(**kwargs)\n",
    "        self.covar_module.initialize_from_data(train_x, train_y, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training data and Build GP Model\n",
    "Generate points in [0, 5] and a sine wave to serve as the response, then split into training and test data.\n",
    "\n",
    "Pass this data into the GP model from above along with a likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 200\n",
    "split = 150\n",
    "full_x = torch.linspace(0, 5, nx)\n",
    "full_y = torch.sin(2 * full_x)\n",
    "\n",
    "train_x = full_x[:split]\n",
    "train_y = full_y[:split]\n",
    "\n",
    "test_x = full_x[(split - nx):]\n",
    "test_y = full_y[(split - nx):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Pretrain On\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_prior=gpytorch.priors.SmoothedBoxPrior(1e-8, 1e-4))\n",
    "model = SpectralModel(train_x, train_y, likelihood, nomg=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up sampling factories\n",
    "\n",
    "In the inference procedure we consider fixing the latent GP observation and doing gradient descent updates on the hyperparameters, then fixing the hyperparameters and using elliptical slice sampling to update the latent GP. \n",
    "\n",
    "The `ss_factory` generates a \"factory\" that fixes the latent GP and computes the loss function of the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Alternating Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20\n",
    "ess_iters = 10\n",
    "optim_iters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-18.1137, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-17.8779, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-17.6377, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-17.4017, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-17.1742, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Seconds for Iteration 0 : 0.7656238079071045\n",
      "Loss is:  tensor(-16.7839, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-16.5562, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-16.3355, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-16.1146, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.9021, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Seconds for Iteration 1 : 0.8067405223846436\n",
      "Loss is:  tensor(-15.6566, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.4383, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.2311, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.0225, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-14.8192, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Seconds for Iteration 2 : 0.6222925186157227\n",
      "Loss is:  tensor(-14.7524, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-14.5590, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-14.3612, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-14.1659, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-13.9806, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Seconds for Iteration 3 : 0.5913341045379639\n",
      "Loss is:  tensor(-13.6972, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-13.5102, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-13.3279, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-13.1434, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.9644, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Seconds for Iteration 4 : 0.6423890590667725\n",
      "Loss is:  tensor(-12.8715, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.6937, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.5258, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.3546, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.1904, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Seconds for Iteration 5 : 0.571577787399292\n",
      "Loss is:  tensor(-12.0541, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-11.8866, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-11.7295, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-11.5698, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-11.4159, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Seconds for Iteration 6 : 0.6196427345275879\n",
      "Loss is:  tensor(-11.1657, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-11.0076, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.8570, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.7084, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.5640, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Seconds for Iteration 7 : 0.601496696472168\n",
      "Loss is:  tensor(-10.3677, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.2271, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.0872, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.9474, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.8058, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Seconds for Iteration 8 : 0.7278518676757812\n",
      "Loss is:  tensor(-9.6730, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.5396, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.4048, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.2810, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-9.1491, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Seconds for Iteration 9 : 0.607414960861206\n",
      "Loss is:  tensor(-9.0623, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.9406, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.8133, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.6935, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.5762, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Seconds for Iteration 10 : 0.6909606456756592\n",
      "Loss is:  tensor(-8.4609, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.3478, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.2317, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.1173, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.0116, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Seconds for Iteration 11 : 0.6547472476959229\n",
      "Loss is:  tensor(-7.8416, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.7298, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.6273, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.5193, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.4187, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Seconds for Iteration 12 : 0.925419807434082\n",
      "Loss is:  tensor(-7.3269, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.2233, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.1239, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-7.0248, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.9284, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Seconds for Iteration 13 : 0.8727917671203613\n",
      "Loss is:  tensor(-6.9016, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.8021, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.7146, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.6199, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.5286, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Seconds for Iteration 14 : 0.7156338691711426\n",
      "Loss is:  tensor(-6.3999, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.3116, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.2266, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.1437, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.0607, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Seconds for Iteration 15 : 0.5733871459960938\n",
      "Loss is:  tensor(-5.9834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.9031, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.8187, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.7430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.6655, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Seconds for Iteration 16 : 0.6731405258178711\n",
      "Loss is:  tensor(-5.5493, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.4722, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.3920, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.3152, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.2459, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Seconds for Iteration 17 : 0.7397160530090332\n",
      "Loss is:  tensor(-5.2372, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.1649, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.0952, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.0234, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.9581, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Seconds for Iteration 18 : 0.7704672813415527\n",
      "Loss is:  tensor(-4.8558, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.7896, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.7176, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.6575, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.5950, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Seconds for Iteration 19 : 0.6308083534240723\n"
     ]
    }
   ],
   "source": [
    "alt_sampler = spectralgp.samplers.AlternatingSampler(\n",
    "    [model], [likelihood], \n",
    "    spectralgp.sampling_factories.ss_factory, [spectralgp.sampling_factories.ess_factory],\n",
    "    totalSamples=n_iters, numInnerSamples=ess_iters, numOuterSamples=optim_iters\n",
    "    )\n",
    "alt_sampler.run();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the outputs\n",
    "The alternating sampler provides a good set of hyperparamters (already attached to the model) and samples of the spectral density of the covariance function.\n",
    "\n",
    "To generate predictions we look at the last samples of the alterntating sampler and generate covariance functions from each of these samples. \n",
    "\n",
    "In the below plotting we show 10 predictions: one prediction for each of the last 10 sampled log-spectral densities taken from the alternating sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised RMSE: 0.1447228640317917\n",
      "Normalised RMSE: 0.15227337181568146\n",
      "Normalised RMSE: 0.14776091277599335\n",
      "Normalised RMSE: 0.1967766135931015\n",
      "Normalised RMSE: 0.16641126573085785\n",
      "Normalised RMSE: 0.16823796927928925\n",
      "Normalised RMSE: 0.17161661386489868\n",
      "Normalised RMSE: 0.16461233794689178\n",
      "Normalised RMSE: 0.16061799228191376\n",
      "Normalised RMSE: 0.1534246951341629\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "n_samples = 10\n",
    "spectrum_samples = alt_sampler.gsampled[0][0,:, -10:].detach()\n",
    "\n",
    "predictions = torch.zeros(len(full_x), 10) # predictions for each sample\n",
    "upper_bds = torch.zeros(len(full_x), 10) # upper conf. bd for each sample\n",
    "lower_bds = torch.zeros(len(full_x), 10) # lower conf. bd for each sample\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ii in range(n_samples):\n",
    "        model.covar_module.set_latent_params(spectrum_samples[:, ii])\n",
    "        model.set_train_data(train_x, train_y) # to clear out the cache\n",
    "        pred_dist = model(full_x) \n",
    "        lower_bds[:, ii], upper_bds[:, ii] = pred_dist.confidence_region()\n",
    "        predictions[:, ii] = pred_dist.mean\n",
    "        d = pred_dist.mean - full_y\n",
    "        test_rmse = torch.sqrt(torch.mean(torch.pow(d, 2)))\n",
    "        print(\"Normalised RMSE: {}\".format(test_rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 100, 10])\n",
      "FULL DATASET: Normalised RMSE: 0.1447228640317917\n",
      "FULL DATASET: Normalised RMSE: 0.15227337181568146\n",
      "FULL DATASET: Normalised RMSE: 0.14776091277599335\n",
      "FULL DATASET: Normalised RMSE: 0.1967766135931015\n",
      "FULL DATASET: Normalised RMSE: 0.16641126573085785\n",
      "FULL DATASET: Normalised RMSE: 0.16823796927928925\n",
      "FULL DATASET: Normalised RMSE: 0.17161661386489868\n",
      "FULL DATASET: Normalised RMSE: 0.16461233794689178\n",
      "FULL DATASET: Normalised RMSE: 0.16061799228191376\n",
      "FULL DATASET: Normalised RMSE: 0.1534246951341629\n"
     ]
    }
   ],
   "source": [
    "print(len(alt_sampler.fgsampled))\n",
    "print(alt_sampler.fgsampled[0].shape)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in range(0,alt_sampler.fgsampled[0].shape[-1]):\n",
    "        model.covar_module.set_latent_params(alt_sampler.fgsampled[0][0, :, x])\n",
    "        model.set_train_data(train_x, train_y) # to clear out the cache\n",
    "        pred_dist = model(full_x) \n",
    "        d = pred_dist.mean - full_y\n",
    "        test_rmse = torch.sqrt(torch.mean(torch.pow(d, 2)))\n",
    "        print(\"FULL DATASET: Normalised RMSE: {}\".format(test_rmse))\n",
    "        \n",
    "#         pred_dist = model(test_x) \n",
    "#         d = pred_dist.mean - test_y\n",
    "#         test_rmse = torch.sqrt(torch.mean(torch.pow(d, 2)))\n",
    "#         print(\"TEST DATASET: Normalised RMSE: {}\".format(test_rmse))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Generate the Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_style(\"whitegrid\")\n",
    "colors = cm.get_cmap(\"tab10\")\n",
    "## plot the predictions ##\n",
    "plt.plot(full_x.numpy(), predictions[:, 0].detach().numpy(), label=\"Predictions\",\n",
    "         color=colors(0), linewidth=2)\n",
    "plt.plot(full_x.numpy(), predictions.detach().numpy(), linewidth=2, \n",
    "         color=colors(0))\n",
    "\n",
    "## Shade region +/- 2 SD around the mean ##\n",
    "plt.fill_between(full_x.numpy(), lower_bds[:, 0].detach().numpy(), \n",
    "                 upper_bds[:, 0].detach().numpy(),\n",
    "                 color=colors(0), alpha=0.03, label = r\"$\\pm 2$ SD\")\n",
    "for ii in range(n_samples):\n",
    "    plt.fill_between(full_x.numpy(), lower_bds[:, ii].detach().numpy(), \n",
    "                     upper_bds[:, ii].detach().numpy(), \n",
    "                     color=colors(0), alpha=0.03)\n",
    "    \n",
    "## plot data ##\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), color=colors(1),\n",
    "        linewidth=2, label=\"Train Data\")\n",
    "plt.plot(test_x.numpy(), test_y.numpy(), color=colors(1),\n",
    "        linestyle=\"None\", marker=\".\", markersize=12,\n",
    "        label=\"Test Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Predictions and Data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.covar_module.omega.numpy(), spectrum_samples.exp().numpy(), label = 'Posterior Samples')\n",
    "plt.xlabel('Omega')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim((0, 7))\n",
    "plt.ylim((0,1))\n",
    "plt.vlines(2/(2*3.14159),ymin=0, ymax=10, label = 'True Period')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.trapz(spectrum_samples.exp().t().numpy(), model.covar_module.omega.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
